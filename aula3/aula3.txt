Fivetran connectors

APIs de sistemas tradicionais de ingestão de dados mudam com frequência. Para evitar alguém dando manutenção nisso com frequência, o Fivetran / Stitch facilitam a etapa de ingestão de dados. 

1.
2.
3.
4.
5.
6.

Não acessamos banco de produção para fazer queries para não sobrecarregá-los com queries de análise de dados

Bucket costuma armazenar:
- parquet vindo de Db (Postgres)
- eventos; e.g. cliques de usuário; em json vindo por streaming
- csvs de orquestrador (Airflow)

Boa prática de armazenamento:

RAW (DEs)
bucket_name/fonte/extracted_at_date/files*
		  PARTIÇÃO

Camada Job é processada pelo Spark

PROCESSED (DEs + DSs)
bucket_name/fonte/data=yyyy-mm-dd/*.parquet

Data é a que usaremos na query
Camda processed é processada por Spark, Glue Jobs, ou DBT

CURATED (semelhante ao que seriam as nossas views) (DEs + DSs + DAs)

- Dados seguem regras de negócio para facilitar visualização
- Quem aplica regras de negócios nessa camada são os analytic engineers
- Dados estão modelados (Data Marts) para servir ferramentas de BI

Ferramentas de BI e DW usam essa camada; Amazon EMR, Redshift, Presto, Athena, etc

Criar S3 raw / S3 processed / S3 Curated: No bucket processed, os dados estão otimizados para leitura

Separar em buckets raw e processed é uma boa prática. Ajuda a isolar os ambientes e facilita usar o IAM. 

bronze/silver/gold data: Convenção de nomes do databricks

Os arquivos no banco processed ficam sempre salvos como parquet.

Vantagens do parquet:
	- Compressão
	- Metadados
	- Colunar

Athena lê e cataloga dados do bucket
Não mexemos mais com dados que estão no raw

Delta. Formato open source da Databricks construído sobre o parquet. Adiciona-se maneira de versionar esses arquivos. 

Particionamento em Data Lake:

O particionamento de dados ajuda a reduzir custos de processamento e a limitar a quantidade de dados escaneados pelas queries. 
150 a 200 Mb é o tamanho ideal para arquivos parquet (rule of thumbs)



			AWS GLUE - Catálogo de dados
			
Armazenamento persistente de metadados. 




Rever parte do aws CDK (antes de POO). ~19:00 e 19:40. 
Ver classes de definição do datalake

CDK serve para definir IaC via código. Boto é mais baixo nível. 


			Delta Lake
			
Transactional Storage Layer to some Cloud Storage Solution. Adds ACID property to distributed files, data versioning and roolback. 
	ACID on Spark
	Scalable to any scale
	Streaming and Batch unification: Easy to put on lambda architecture
	Delta 10 to 100 times faster than parquet on Spark
	
	
			Big Query
			
Armazenamento no S3, compactação em parquet por conta do Google.
SQL as a Service
Suporte para batch ou stream
Após 90 dias sem atualização, deixa em camada mais fria


Dúvidas:

Amazon CDK
Versionamento?


